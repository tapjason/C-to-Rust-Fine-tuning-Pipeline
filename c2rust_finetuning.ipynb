{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5d18cb-ede6-4b15-b01e-c82c478468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import requests\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"OpenAI package not installed.\")\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "CONFIG = {\n",
    "    'max_projects': 10,\n",
    "    'min_examples': 10,\n",
    "    'max_examples': 25,\n",
    "    'max_tokens_per_example': 1500,\n",
    "    'train_split': 0.8,\n",
    "    'target_model': 'gpt-4o-mini-2024-07-18',\n",
    "    'output_dir': 'o3_fine_tuning_outputs',\n",
    "    'cbench_path': 'data/CRUST_bench/CBench',\n",
    "    'rbench_path': 'data/CRUST_bench/RBench'\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG['output_dir']).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461645a7-ad38-4674-bf89-4d099e475abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ZIP contents...\n",
      "Dataset extracted to data\\CRUST-bench\n",
      "C code path: data\\CRUST-bench\\CRUST_bench\\CBench\n",
      "Rust code path: data\\CRUST-bench\\CRUST_bench\\RBench\n"
     ]
    }
   ],
   "source": [
    "def setup_crust_bench():\n",
    "    data_dir = Path(\"data/CRUST-bench\")\n",
    "    zip_url = \"https://github.com/anirudhkhatry/CRUST-bench/raw/main/datasets/CRUST_bench.zip\"\n",
    "    zip_path = data_dir / \"CRUST_bench.zip\"\n",
    "\n",
    "    if not data_dir.exists():\n",
    "        print(\"Creating data directory...\")\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not zip_path.exists():\n",
    "        print(\"Downloading CRUST_bench.zip...\")\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(zip_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download complete.\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download dataset: HTTP {response.status_code}\")\n",
    "\n",
    "    print(\"Extracting ZIP contents...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "\n",
    "    print(f\"Dataset extracted to {data_dir}\")\n",
    "    return data_dir\n",
    "\n",
    "# Setup dataset\n",
    "try:\n",
    "    DATASET_PATH = setup_crust_bench()\n",
    "    CBENCH_PATH = DATASET_PATH / \"CRUST_bench\" / \"CBench\"\n",
    "    RBENCH_PATH = DATASET_PATH / \"CRUST_bench\" / \"RBench\"\n",
    "    print(f\"C code path: {CBENCH_PATH}\")\n",
    "    print(f\"Rust code path: {RBENCH_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not setup dataset: {e}\")\n",
    "    CBENCH_PATH = Path(\"CBench\")\n",
    "    RBENCH_PATH = Path(\"RBench\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2fbb827-950c-47da-8db5-ad16a47939af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "Loading CRUST-bench with extreme selectivity...\n",
      "Dataset overview:\n",
      "   C projects: 100\n",
      "   Rust projects: 100\n",
      "   Common projects: 80\n",
      "Selected projects: ['CircularBuffer', 'amp', 'FastHamming', 'ted', 'totp', 'tisp', 'satc', 'razz_simulation', 'NandC', 'rubiksolver']\n",
      "Candidate examples: 37\n",
      "Selected examples: 25\n",
      "Quality range: 4.0 - 10.0\n",
      "\n",
      "SELECTED EXAMPLES FOR O3:\n",
      "--------------------------------------------------\n",
      "1. Project: rubiksolver\n",
      "   Files: rubik_model_tests.c → hash.rs\n",
      "   Size: 68 C lines → 29 Rust lines\n",
      "   Tokens: 908\n",
      "   Quality: 10.0\n",
      "2. Project: rubiksolver\n",
      "   Files: rubik_model_tests.c → heap.rs\n",
      "   Size: 68 C lines → 30 Rust lines\n",
      "   Tokens: 835\n",
      "   Quality: 10.0\n",
      "3. Project: rubiksolver\n",
      "   Files: hash\\hash_tests.c → hash.rs\n",
      "   Size: 34 C lines → 29 Rust lines\n",
      "   Tokens: 572\n",
      "   Quality: 10.0\n",
      "4. Project: rubiksolver\n",
      "   Files: hash\\hash_tests.c → heap.rs\n",
      "   Size: 34 C lines → 30 Rust lines\n",
      "   Tokens: 500\n",
      "   Quality: 10.0\n",
      "5. Project: rubiksolver\n",
      "   Files: heap\\heap_tests.c → hash.rs\n",
      "   Size: 53 C lines → 29 Rust lines\n",
      "   Tokens: 650\n",
      "   Quality: 10.0\n",
      "6. Project: rubiksolver\n",
      "   Files: heap\\heap_tests.c → heap.rs\n",
      "   Size: 53 C lines → 30 Rust lines\n",
      "   Tokens: 577\n",
      "   Quality: 10.0\n",
      "7. Project: rubiksolver\n",
      "   Files: hash\\hash.c → hash.rs\n",
      "   Size: 125 C lines → 29 Rust lines\n",
      "   Tokens: 1191\n",
      "   Quality: 8.0\n",
      "8. Project: rubiksolver\n",
      "   Files: hash\\hash.c → heap.rs\n",
      "   Size: 125 C lines → 30 Rust lines\n",
      "   Tokens: 1118\n",
      "   Quality: 8.0\n",
      "9. Project: rubiksolver\n",
      "   Files: heap\\heap.c → hash.rs\n",
      "   Size: 122 C lines → 29 Rust lines\n",
      "   Tokens: 996\n",
      "   Quality: 8.0\n",
      "10. Project: rubiksolver\n",
      "   Files: heap\\heap.c → heap.rs\n",
      "   Size: 122 C lines → 30 Rust lines\n",
      "   Tokens: 923\n",
      "   Quality: 8.0\n",
      "11. Project: FastHamming\n",
      "   Files: test\\dummy.c → fast_hamming.rs\n",
      "   Size: 24 C lines → 18 Rust lines\n",
      "   Tokens: 211\n",
      "   Quality: 7.0\n",
      "12. Project: FastHamming\n",
      "   Files: test\\log2uint8.c → fast_hamming.rs\n",
      "   Size: 60 C lines → 18 Rust lines\n",
      "   Tokens: 414\n",
      "   Quality: 7.0\n",
      "13. Project: tisp\n",
      "   Files: tib\\os.c → core.rs\n",
      "   Size: 94 C lines → 55 Rust lines\n",
      "   Tokens: 1053\n",
      "   Quality: 7.0\n",
      "14. Project: tisp\n",
      "   Files: tib\\os.c → io.rs\n",
      "   Size: 94 C lines → 22 Rust lines\n",
      "   Tokens: 786\n",
      "   Quality: 7.0\n",
      "15. Project: tisp\n",
      "   Files: tib\\os.c → math.rs\n",
      "   Size: 94 C lines → 37 Rust lines\n",
      "   Tokens: 907\n",
      "   Quality: 7.0\n",
      "16. Project: tisp\n",
      "   Files: tib\\os.c → os.rs\n",
      "   Size: 94 C lines → 19 Rust lines\n",
      "   Tokens: 774\n",
      "   Quality: 7.0\n",
      "17. Project: tisp\n",
      "   Files: tib\\os.c → string.rs\n",
      "   Size: 94 C lines → 20 Rust lines\n",
      "   Tokens: 789\n",
      "   Quality: 7.0\n",
      "18. Project: CircularBuffer\n",
      "   Files: tests\\test.c → circular_buffer.rs\n",
      "   Size: 90 C lines → 49 Rust lines\n",
      "   Tokens: 940\n",
      "   Quality: 6.0\n",
      "19. Project: amp\n",
      "   Files: src\\amp.c → amp.rs\n",
      "   Size: 110 C lines → 41 Rust lines\n",
      "   Tokens: 693\n",
      "   Quality: 6.0\n",
      "20. Project: amp\n",
      "   Files: tests\\test.c → amp.rs\n",
      "   Size: 38 C lines → 41 Rust lines\n",
      "   Tokens: 413\n",
      "   Quality: 5.0\n",
      "21. Project: rubiksolver\n",
      "   Files: solve_rubik.c → hash.rs\n",
      "   Size: 104 C lines → 29 Rust lines\n",
      "   Tokens: 1088\n",
      "   Quality: 5.0\n",
      "22. Project: rubiksolver\n",
      "   Files: solve_rubik.c → heap.rs\n",
      "   Size: 104 C lines → 30 Rust lines\n",
      "   Tokens: 1015\n",
      "   Quality: 5.0\n",
      "23. Project: totp\n",
      "   Files: main.c → std.rs\n",
      "   Size: 28 C lines → 12 Rust lines\n",
      "   Tokens: 200\n",
      "   Quality: 4.0\n",
      "24. Project: totp\n",
      "   Files: main.c → totp.rs\n",
      "   Size: 28 C lines → 32 Rust lines\n",
      "   Tokens: 336\n",
      "   Quality: 4.0\n",
      "25. Project: totp\n",
      "   Files: std.c → std.rs\n",
      "   Size: 34 C lines → 12 Rust lines\n",
      "   Tokens: 171\n",
      "   Quality: 4.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_example_quality(c_code, rust_code):\n",
    "    \"\"\"\n",
    "    Score example quality for O3 selection\n",
    "    Higher scores = better training examples for code translation\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    c_lower = c_code.lower()\n",
    "    rust_lower = rust_code.lower()\n",
    "    \n",
    "    # Memory management patterns (high value for translation)\n",
    "    if any(pattern in c_lower for pattern in ['malloc', 'free', 'realloc', 'calloc']):\n",
    "        score += 3\n",
    "    \n",
    "    # Good Rust safety patterns\n",
    "    if any(pattern in rust_lower for pattern in ['vec<', 'box<', 'result<', 'option<']):\n",
    "        score += 2\n",
    "    \n",
    "    # Idiomatic Rust structures\n",
    "    if any(pattern in rust_lower for pattern in ['impl', 'pub fn', 'struct', 'enum']):\n",
    "        score += 2\n",
    "    \n",
    "    # Appropriate complexity (not too simple, not too complex)\n",
    "    c_lines = c_code.count('\\n') + 1\n",
    "    rust_lines = rust_code.count('\\n') + 1\n",
    "    if 20 <= c_lines <= 100 and 10 <= rust_lines <= 80:\n",
    "        score += 2\n",
    "    \n",
    "    # Error handling patterns\n",
    "    if any(pattern in rust_lower for pattern in ['?', '.unwrap', '.expect', 'match']):\n",
    "        score += 1\n",
    "    \n",
    "    # Documentation (bonus)\n",
    "    if '///' in rust_code or '/*' in rust_code:\n",
    "        score += 1\n",
    "    \n",
    "    # C-Rust interface compatibility\n",
    "    if 'extern \"c\"' in rust_lower or '#[no_mangle]' in rust_lower:\n",
    "        score += 1\n",
    "\n",
    "    if 'unsafe' in rust_lower:\n",
    "        score -= 1\n",
    "    if '.unwrap()' in rust_lower and not '// safe unwrap' in rust_lower:\n",
    "        score -= 0.5\n",
    "\n",
    "    return score\n",
    "\n",
    "def load_crust_bench_minimal():\n",
    "    \"\"\"\n",
    "    Load minimal high-quality examples from CRUST-bench\n",
    "    Optimized for O3 cost constraints\n",
    "    \"\"\"\n",
    "    \n",
    "    cbench_path = Path(CONFIG['cbench_path'])\n",
    "    rbench_path = Path(CONFIG['rbench_path'])\n",
    "    \n",
    "    print(f\"Loading CRUST-bench with extreme selectivity...\")\n",
    "    \n",
    "    # Verify paths\n",
    "    if not cbench_path.exists() or not rbench_path.exists():\n",
    "        print(f\"CRUST-bench dataset not found!\")\n",
    "        print(f\"Expected structure:\")\n",
    "        print(f\"  {cbench_path}\")\n",
    "        print(f\"  {rbench_path}\")\n",
    "        return create_synthetic_examples()  # Fallback for demo\n",
    "    \n",
    "    # Get project intersection\n",
    "    c_projects = [d.name for d in cbench_path.iterdir() if d.is_dir()]\n",
    "    r_projects = [d.name for d in rbench_path.iterdir() if d.is_dir()]\n",
    "    common_projects = set(c_projects) & set(r_projects)\n",
    "    \n",
    "    print(f\"Dataset overview:\")\n",
    "    print(f\"   C projects: {len(c_projects)}\")\n",
    "    print(f\"   Rust projects: {len(r_projects)}\")\n",
    "    print(f\"   Common projects: {len(common_projects)}\")\n",
    "    \n",
    "    # Priority projects for quality\n",
    "    priority_projects = ['CircularBuffer', 'amp', 'FastHamming', 'ted', 'totp']\n",
    "    selected_projects = [p for p in priority_projects if p in common_projects]\n",
    "    \n",
    "    # Fill remaining slots if needed\n",
    "    remaining = list(common_projects - set(selected_projects))\n",
    "    selected_projects.extend(remaining[:CONFIG['max_projects'] - len(selected_projects)])\n",
    "    \n",
    "    print(f\"Selected projects: {selected_projects[:CONFIG['max_projects']]}\")\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Process each project\n",
    "    for project_name in selected_projects[:CONFIG['max_projects']]:\n",
    "        c_project_dir = cbench_path / project_name\n",
    "        r_project_dir = rbench_path / project_name\n",
    "        \n",
    "        # Load C files with size filtering\n",
    "        c_files = {}\n",
    "        for c_file in c_project_dir.rglob(\"*.c\"):\n",
    "            try:\n",
    "                with open(c_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read().strip()\n",
    "                    lines = content.count('\\n') + 1\n",
    "                    # Strict filtering for O3\n",
    "                    if 20 <= lines <= 150 and len(content) > 200:\n",
    "                        rel_path = c_file.relative_to(c_project_dir)\n",
    "                        c_files[str(rel_path)] = content\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Load Rust interface files\n",
    "        interfaces_dir = r_project_dir / \"src\" / \"interfaces\"\n",
    "        rust_files = {}\n",
    "        \n",
    "        if interfaces_dir.exists():\n",
    "            for rust_file in interfaces_dir.rglob(\"*.rs\"):\n",
    "                try:\n",
    "                    with open(rust_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read().strip()\n",
    "                        lines = content.count('\\n') + 1\n",
    "                        if 10 <= lines <= 100 and len(content) > 100:\n",
    "                            rel_path = rust_file.relative_to(interfaces_dir)\n",
    "                            rust_files[str(rel_path)] = content\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Create and score candidates\n",
    "        for c_path, c_content in c_files.items():\n",
    "            for rust_path, rust_content in rust_files.items():\n",
    "                estimated_tokens = len(c_content + rust_content) // 4\n",
    "                \n",
    "                if estimated_tokens <= CONFIG['max_tokens_per_example']:\n",
    "                    quality_score = calculate_example_quality(c_content, rust_content)\n",
    "                    \n",
    "                    candidates.append({\n",
    "                        'project': project_name,\n",
    "                        'c_file': c_path,\n",
    "                        'rust_file': rust_path,\n",
    "                        'c_code': c_content,\n",
    "                        'rust_code': rust_content,\n",
    "                        'c_lines': c_content.count('\\n') + 1,\n",
    "                        'rust_lines': rust_content.count('\\n') + 1,\n",
    "                        'estimated_tokens': estimated_tokens,\n",
    "                        'quality_score': quality_score\n",
    "                    })\n",
    "    \n",
    "    # Select best examples by quality\n",
    "    candidates.sort(key=lambda x: x['quality_score'], reverse=True)\n",
    "    selected = candidates[:CONFIG['max_examples']]\n",
    "    \n",
    "    print(f\"Candidate examples: {len(candidates)}\")\n",
    "    print(f\"Selected examples: {len(selected)}\")\n",
    "    \n",
    "    if selected:\n",
    "        print(f\"Quality range: {min(ex['quality_score'] for ex in selected):.1f} - {max(ex['quality_score'] for ex in selected):.1f}\")\n",
    "    \n",
    "    return selected\n",
    "\n",
    "def create_synthetic_examples():\n",
    "    \"\"\"\n",
    "    Create synthetic examples if CRUST-bench not available (for demo purposes)\n",
    "    \"\"\"\n",
    "    print(\"📝 Creating synthetic examples for demonstration...\")\n",
    "    \n",
    "    synthetic_examples = [\n",
    "        {\n",
    "            'project': 'SyntheticCircularBuffer',\n",
    "            'c_file': 'circular_buffer.c',\n",
    "            'rust_file': 'circular_buffer.rs',\n",
    "            'c_code': '''#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdbool.h>\n",
    "\n",
    "typedef struct {\n",
    "    int* buffer;\n",
    "    size_t size;\n",
    "    size_t head;\n",
    "    size_t tail;\n",
    "    bool full;\n",
    "} CircularBuffer;\n",
    "\n",
    "CircularBuffer* cb_create(size_t size) {\n",
    "    CircularBuffer* cb = malloc(sizeof(CircularBuffer));\n",
    "    if (!cb) return NULL;\n",
    "    \n",
    "    cb->buffer = malloc(size * sizeof(int));\n",
    "    if (!cb->buffer) {\n",
    "        free(cb);\n",
    "        return NULL;\n",
    "    }\n",
    "    \n",
    "    cb->size = size;\n",
    "    cb->head = 0;\n",
    "    cb->tail = 0;\n",
    "    cb->full = false;\n",
    "    \n",
    "    return cb;\n",
    "}\n",
    "\n",
    "bool cb_write(CircularBuffer* cb, int data) {\n",
    "    if (!cb) return false;\n",
    "    \n",
    "    cb->buffer[cb->head] = data;\n",
    "    \n",
    "    if (cb->full) {\n",
    "        cb->tail = (cb->tail + 1) % cb->size;\n",
    "    }\n",
    "    \n",
    "    cb->head = (cb->head + 1) % cb->size;\n",
    "    cb->full = (cb->head == cb->tail);\n",
    "    \n",
    "    return true;\n",
    "}\n",
    "\n",
    "void cb_destroy(CircularBuffer* cb) {\n",
    "    if (cb) {\n",
    "        free(cb->buffer);\n",
    "        free(cb);\n",
    "    }\n",
    "}''',\n",
    "            'rust_code': '''use std::vec::Vec;\n",
    "\n",
    "/// A circular buffer implementation in safe Rust\n",
    "pub struct CircularBuffer {\n",
    "    buffer: Vec<i32>,\n",
    "    head: usize,\n",
    "    tail: usize,\n",
    "    full: bool,\n",
    "}\n",
    "\n",
    "impl CircularBuffer {\n",
    "    /// Creates a new circular buffer with the specified capacity\n",
    "    pub fn new(size: usize) -> Result<Self, &'static str> {\n",
    "        if size == 0 {\n",
    "            return Err(\"Size must be greater than 0\");\n",
    "        }\n",
    "        \n",
    "        Ok(CircularBuffer {\n",
    "            buffer: vec![0; size],\n",
    "            head: 0,\n",
    "            tail: 0,\n",
    "            full: false,\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    /// Writes data to the buffer\n",
    "    pub fn write(&mut self, data: i32) -> bool {\n",
    "        self.buffer[self.head] = data;\n",
    "        \n",
    "        if self.full {\n",
    "            self.tail = (self.tail + 1) % self.buffer.len();\n",
    "        }\n",
    "        \n",
    "        self.head = (self.head + 1) % self.buffer.len();\n",
    "        self.full = self.head == self.tail;\n",
    "        \n",
    "        true\n",
    "    }\n",
    "    \n",
    "    /// Returns the size of the buffer\n",
    "    pub fn size(&self) -> usize {\n",
    "        self.buffer.len()\n",
    "    }\n",
    "}\n",
    "\n",
    "impl Drop for CircularBuffer {\n",
    "    fn drop(&mut self) {\n",
    "        // Rust automatically handles deallocation\n",
    "    }\n",
    "}''',\n",
    "            'c_lines': 47,\n",
    "            'rust_lines': 44,\n",
    "            'estimated_tokens': 875,\n",
    "            'quality_score': 8.0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return synthetic_examples\n",
    "\n",
    "# Load the dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "dataset = load_crust_bench_minimal()\n",
    "\n",
    "if not dataset:\n",
    "    print(\"No examples found, using synthetic data for demo\")\n",
    "    dataset = create_synthetic_examples()\n",
    "\n",
    "print(f\"\\nSELECTED EXAMPLES FOR O3:\")\n",
    "print(\"-\" * 50)\n",
    "for i, example in enumerate(dataset, 1):\n",
    "    print(f\"{i}. Project: {example['project']}\")\n",
    "    print(f\"   Files: {example['c_file']} → {example['rust_file']}\")\n",
    "    print(f\"   Size: {example['c_lines']} C lines → {example['rust_lines']} Rust lines\")\n",
    "    print(f\"   Tokens: {example['estimated_tokens']}\")\n",
    "    print(f\"   Quality: {example['quality_score']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee3b478-37d5-493a-b1d4-aeac670b3488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGH-QUALITY TRANSLATION EXAMPLES\n",
      "============================================================\n",
      "\n",
      "EXAMPLE 1: RUBIKSOLVER\n",
      "   Quality Score: 10.0/10\n",
      "   Size: 68 C lines → 29 Rust lines\n",
      "   File: rubik_model_tests.c → hash.rs\n",
      "   Tokens: ~908\n",
      "\n",
      "C CODE:\n",
      "────────────────────────────────────────\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <assert.h>\n",
      "\n",
      "#include \"rubik_model.h\"\n",
      "\n",
      "int main()\n",
      "{\n",
      "  printf(\"Running rubik_model tests\\n\");\n",
      "  \n",
      "  printf(\"Testing REAR...\\n\");\n",
      "  assert_verbose(REAR(RED) == ORANGE);\n",
      "  assert_verbose(REAR(GREEN) == YELLOW);\n",
      "  assert_verbose(REAR(BLUE) == WHITE);\n",
      "  assert_verbose(REAR(REAR(RED)) == RED);\n",
      "  assert_verbose(REAR(REAR(GREEN)) == GREEN);\n",
      "  assert_verbose(REAR(REAR(BLUE)) == BLUE);\n",
      "  \n",
      "  // Call function to test private functions(adjacent_cw, adjacent_ccw,....\n",
      "\n",
      "RUST CODE:\n",
      "────────────────────────────────────────\n",
      "/// A simple hash table using a vector of buckets.\n",
      "/// The table uses a provided hash function (which returns a u32)\n",
      "/// and a caller‑supplied equality function for comparing elements.\n",
      "pub struct Hash<T> {\n",
      "    buckets: Vec<Vec<T>>,\n",
      "    hash_function: Box<dyn Fn(&T) -> u32>,\n",
      "    max_key: u32,\n",
      "}\n",
      "impl<T> Hash<T> {\n",
      "    /// Create a new hash table with `max_key` buckets.\n",
      "    /// The `hash_function` is used to compute a hash for each element.\n",
      "    pub fn new(max_key: u32, hash_function: impl Fn(&T) -> ...\n",
      "\n",
      "QUALITY INDICATORS:\n",
      "   • Memory management patterns (malloc/free)\n",
      "   • Safe Rust types (Vec, Box, Result, Option)\n",
      "   • Idiomatic Rust structures\n",
      "   • Memory-safe implementation\n",
      "   • Documentation comments\n",
      "\n",
      "============================================================\n",
      "\n",
      "EXAMPLE 2: RUBIKSOLVER\n",
      "   Quality Score: 10.0/10\n",
      "   Size: 68 C lines → 30 Rust lines\n",
      "   File: rubik_model_tests.c → heap.rs\n",
      "   Tokens: ~835\n",
      "\n",
      "C CODE:\n",
      "────────────────────────────────────────\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <assert.h>\n",
      "\n",
      "#include \"rubik_model.h\"\n",
      "\n",
      "int main()\n",
      "{\n",
      "  printf(\"Running rubik_model tests\\n\");\n",
      "  \n",
      "  printf(\"Testing REAR...\\n\");\n",
      "  assert_verbose(REAR(RED) == ORANGE);\n",
      "  assert_verbose(REAR(GREEN) == YELLOW);\n",
      "  assert_verbose(REAR(BLUE) == WHITE);\n",
      "  assert_verbose(REAR(REAR(RED)) == RED);\n",
      "  assert_verbose(REAR(REAR(GREEN)) == GREEN);\n",
      "  assert_verbose(REAR(REAR(BLUE)) == BLUE);\n",
      "  \n",
      "  // Call function to test private functions(adjacent_cw, adjacent_ccw,....\n",
      "\n",
      "RUST CODE:\n",
      "────────────────────────────────────────\n",
      "/// A custom binary heap that stores elements in a Vec and uses a custom comparator.\n",
      "pub struct Heap<T> {\n",
      "    data: Vec<T>,\n",
      "    comparator: Box<dyn Fn(&T, &T) -> bool>,\n",
      "}\n",
      "impl<T> Heap<T> {\n",
      "    /// Creates a new heap with the given initial capacity and comparator.\n",
      "    pub fn new(init_size: usize, comparator: impl Fn(&T, &T) -> bool + 'static) -> Self {\n",
      "        Self {\n",
      "            data: Vec::with_capacity(init_size),\n",
      "            comparator: Box::new(comparator),\n",
      "        }\n",
      "    }\n",
      "    /// Returns true...\n",
      "\n",
      "QUALITY INDICATORS:\n",
      "   • Memory management patterns (malloc/free)\n",
      "   • Safe Rust types (Vec, Box, Result, Option)\n",
      "   • Idiomatic Rust structures\n",
      "   • Memory-safe implementation\n",
      "   • Documentation comments\n",
      "\n",
      "============================================================\n",
      "\n",
      "EXAMPLE 3: RUBIKSOLVER\n",
      "   Quality Score: 10.0/10\n",
      "   Size: 34 C lines → 29 Rust lines\n",
      "   File: hash\\hash_tests.c → hash.rs\n",
      "   Tokens: ~572\n",
      "\n",
      "C CODE:\n",
      "────────────────────────────────────────\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <string.h>\n",
      "#include <assert.h>\n",
      "\n",
      "#include \"hash.h\"\n",
      "\n",
      "unsigned int hash_function(const void *element)\n",
      "{\n",
      "  const char *e_str = (const char *)element;\n",
      "  return (unsigned int)(e_str[0]);\n",
      "}\n",
      "\n",
      "bool compare_equal(const void *e1, const void *e2)\n",
      "{\n",
      "  return (strcmp((const char *)e1, (const char *)e2) == 0);\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "  hash_t hash = hash_init(255, hash_function);\n",
      "  assert_verbose(hash_element_exists(hash, \"Hello\", compare_equal) == false);\n",
      "  printf(\"Inse...\n",
      "\n",
      "RUST CODE:\n",
      "────────────────────────────────────────\n",
      "/// A simple hash table using a vector of buckets.\n",
      "/// The table uses a provided hash function (which returns a u32)\n",
      "/// and a caller‑supplied equality function for comparing elements.\n",
      "pub struct Hash<T> {\n",
      "    buckets: Vec<Vec<T>>,\n",
      "    hash_function: Box<dyn Fn(&T) -> u32>,\n",
      "    max_key: u32,\n",
      "}\n",
      "impl<T> Hash<T> {\n",
      "    /// Create a new hash table with `max_key` buckets.\n",
      "    /// The `hash_function` is used to compute a hash for each element.\n",
      "    pub fn new(max_key: u32, hash_function: impl Fn(&T) -> ...\n",
      "\n",
      "QUALITY INDICATORS:\n",
      "   • Memory management patterns (malloc/free)\n",
      "   • Safe Rust types (Vec, Box, Result, Option)\n",
      "   • Idiomatic Rust structures\n",
      "   • Memory-safe implementation\n",
      "   • Documentation comments\n",
      "\n",
      "\n",
      "TRANSLATION APPROACH COMPARISON\n",
      "============================================================\n",
      "SOURCE: rubiksolver - rubik_model_tests.c\n",
      "ORIGINAL C CODE:\n",
      "────────────────────────────────────────\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <assert.h>\n",
      "\n",
      "#include \"rubik_model.h\"\n",
      "\n",
      "int main()\n",
      "{\n",
      "  printf(\"Running rubik_model tests\\n\");\n",
      "  \n",
      "  printf(\"Testing REAR...\\n\");\n",
      "  assert_verbose(REAR(RED) == ORANGE);\n",
      "  assert_verbose(REAR(GREEN) == YELLOW);\n",
      "  assert_verbose(REAR(BLUE) == WHITE);\n",
      "  assert_verbose(REAR(REAR(RED)) == RED);\n",
      "  assert_verbose(REAR(REAR(GREEN)) == GREEN);\n",
      "  assert_verbose(REAR(REAR(BLUE)) == BLUE);\n",
      "  \n",
      "  // Call function to test private functions(adjacent_cw, adjacent_ccw,...)\n",
      "  test_adj_functions();\n",
      "  \n",
      "  printf(\"Testing cube_compare_equal...\\n\");\n",
      "  const color test_cube_data[][8] = {\n",
      "      {RED, RED, RED, RED, RED, RED, RED, RED},\n",
      "      {GREEN, YELLOW, BLUE, ORANGE, ORANGE, YELLOW, GREEN, GREEN},\n",
      "      {BLUE, BLUE, BLUE, ORANGE, ORANGE, GREEN, ORANGE, BLUE},\n",
      "      {WHITE, WHITE, WHITE, GREEN, GREEN, WHITE, YELLOW, BLUE},\n",
      "      {ORANGE, GREEN, BLUE, YELLOW, YELLOW, YELLOW, YELLOW, BLUE},\n",
      "      {YELLOW, ORANGE, WHITE, WHITE, WHITE, WHITE, GREEN, ORANGE}\n",
      "    };\n",
      "  cube_t test_cube = populate_specific(test_cube_data);\n",
      "  cube_t original = populate_specific(test_cube_data);\n",
      "\n",
      "  const color output1_data[][8] = {\n",
      "      {RED, RED, RED, RED, BLUE, ORANGE, ORANGE, RED},\n",
      "      {GREEN, YELLOW, BLUE, ORANGE, ORANGE, YELLOW, GREEN, GREEN},\n",
      "      {BLUE, BLUE, YELLOW, BLUE, WHITE, GREEN, ORANGE, BLUE},\n",
      "      {WHITE, WHITE, WHITE, GREEN, GREEN, WHITE, YELLOW, ORANGE},\n",
      "      {YELLOW, BLUE, ORANGE, GREEN, BLUE, YELLOW, YELLOW, YELLOW},\n",
      "      {RED, RED, RED, WHITE, WHITE, WHITE, GREEN, ORANGE}\n",
      "    };\n",
      "  cube_t output1 = populate_specific(output1_data);\n",
      "\n",
      "  assert_verbose(cube_compare_equal(test_cube, original));\n",
      "  assert_verbose(!cube_compare_equal(test_cube, output1));\n",
      "  \n",
      "  printf(\"Testing SWAP_COLOR...\\n\");\n",
      "  color r = RED, g = GREEN;\n",
      "  SWAP_COLOR(r,g);\n",
      "  assert_verbose(r == GREEN && g == RED);\n",
      "  SWAP_COLOR(r,g);\n",
      "  assert_verbose(r == RED && g == GREEN);\n",
      "  \n",
      "  printf(\"Testing rotate_face...\\n\");\n",
      "  rotate_face(test_cube, YELLOW, CW);\n",
      "  assert_verbose(cube_compare_equal(test_cube, output1));\n",
      "  rotate_face(test_cube, YELLOW, CCW);\n",
      "  assert_verbose(cube_compare_equal(test_cube, original));\n",
      "  \n",
      "  printf(\"Testing find_entropy...\\n\");\n",
      "  assert_verbose(find_entropy(original) == 25);\n",
      "  \n",
      "  cube_free(test_cube);\n",
      "  cube_free(original);\n",
      "  cube_free(output1);\n",
      "  printf(\"rubik_model tests finished successfully\\n\");\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "EXPERT RUST TRANSLATION:\n",
      "────────────────────────────────────────\n",
      "/// A simple hash table using a vector of buckets.\n",
      "/// The table uses a provided hash function (which returns a u32)\n",
      "/// and a caller‑supplied equality function for comparing elements.\n",
      "pub struct Hash<T> {\n",
      "    buckets: Vec<Vec<T>>,\n",
      "    hash_function: Box<dyn Fn(&T) -> u32>,\n",
      "    max_key: u32,\n",
      "}\n",
      "impl<T> Hash<T> {\n",
      "    /// Create a new hash table with `max_key` buckets.\n",
      "    /// The `hash_function` is used to compute a hash for each element.\n",
      "    pub fn new(max_key: u32, hash_function: impl Fn(&T) -> u32 + 'static) -> Self {\n",
      "        unimplemented!()\n",
      "    }\n",
      "    /// Inserts an element if no equal element exists (using the provided compare_equal).\n",
      "    /// Returns true if the element was inserted.\n",
      "    pub fn insert(&mut self, element: T, compare_equal: impl Fn(&T, &T) -> bool) -> bool {\n",
      "        unimplemented!()\n",
      "    }\n",
      "    /// Returns true if an element equal (per compare_equal) to `element` exists.\n",
      "    pub fn element_exists(&self, element: &T, compare_equal: impl Fn(&T, &T) -> bool) -> bool {\n",
      "        unimplemented!()\n",
      "    }\n",
      "    /// Deletes an element (using compare_equal) from the hash table.\n",
      "    /// Returns true if the element was found and removed.\n",
      "    pub fn delete(&mut self, element: &T, compare_equal: impl Fn(&T, &T) -> bool) -> bool {\n",
      "        unimplemented!()\n",
      "    }\n",
      "}\n",
      "\n",
      "WHAT A FINE-TUNED MODEL SHOULD LEARN:\n",
      "────────────────────────────────────────\n",
      "   • C functions → Rust methods with proper visibility\n",
      "\n",
      "\n",
      "QUALITY SCORE DISTRIBUTION\n",
      "============================================================\n",
      "STATISTICS:\n",
      "   Total Examples: 25\n",
      "   Average Quality: 7.20/10\n",
      "   Highest Quality: 10.0/10\n",
      "   Lowest Quality: 4.0/10\n",
      "   Standard Deviation: 1.98\n",
      "\n",
      "QUALITY DISTRIBUTION:\n",
      "   Poor (0-2)      │█                     0 ( 0.0%)\n",
      "   Below Avg (2-4) │█                     0 ( 0.0%)\n",
      "   Average (4-6)   │████                  6 (24.0%)\n",
      "   Good (6-8)      │███████               9 (36.0%)\n",
      "   Excellent (8-10) │███                   4 (16.0%)\n",
      "\n",
      "INSIGHT:\n",
      "   19 examples (76.0%) are high-quality (6+)\n",
      "   These form the core of our training dataset\n"
     ]
    }
   ],
   "source": [
    "def display_high_quality_examples():\n",
    "    \"\"\"\n",
    "    Display the highest quality C-to-Rust translation examples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"HIGH-QUALITY TRANSLATION EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"No dataset available. Using synthetic example for demonstration.\")\n",
    "        return\n",
    "    \n",
    "    # Sort examples by quality score (highest first)\n",
    "    sorted_examples = sorted(dataset, key=lambda x: x['quality_score'], reverse=True)\n",
    "    \n",
    "    # Display top 3 examples\n",
    "    for i, example in enumerate(sorted_examples[:3], 1):\n",
    "        print(f\"\\nEXAMPLE {i}: {example['project'].upper()}\")\n",
    "        print(f\"   Quality Score: {example['quality_score']:.1f}/10\")\n",
    "        print(f\"   Size: {example['c_lines']} C lines → {example['rust_lines']} Rust lines\")\n",
    "        print(f\"   File: {example['c_file']} → {example['rust_file']}\")\n",
    "        print(f\"   Tokens: ~{example['estimated_tokens']}\")\n",
    "        \n",
    "        print(f\"\\nC CODE:\")\n",
    "        print(\"─\" * 40)\n",
    "        print(example['c_code'][:500] + \"...\" if len(example['c_code']) > 500 else example['c_code'])\n",
    "        \n",
    "        print(f\"\\nRUST CODE:\")\n",
    "        print(\"─\" * 40)\n",
    "        print(example['rust_code'][:500] + \"...\" if len(example['rust_code']) > 500 else example['rust_code'])\n",
    "        \n",
    "        # Analyze what makes this example high quality\n",
    "        print(f\"\\nQUALITY INDICATORS:\")\n",
    "        c_lower = example['c_code'].lower()\n",
    "        rust_lower = example['rust_code'].lower()\n",
    "        \n",
    "        indicators = []\n",
    "        if any(pattern in c_lower for pattern in ['malloc', 'free', 'realloc', 'calloc']):\n",
    "            indicators.append(\"• Memory management patterns (malloc/free)\")\n",
    "        if any(pattern in rust_lower for pattern in ['vec<', 'box<', 'result<', 'option<']):\n",
    "            indicators.append(\"• Safe Rust types (Vec, Box, Result, Option)\")\n",
    "        if any(pattern in rust_lower for pattern in ['impl', 'pub fn', 'struct', 'enum']):\n",
    "            indicators.append(\"• Idiomatic Rust structures\")\n",
    "        if any(pattern in rust_lower for pattern in ['?', '.unwrap', '.expect', 'match']):\n",
    "            indicators.append(\"• Proper error handling\")\n",
    "        if 'unsafe' not in rust_lower:\n",
    "            indicators.append(\"• Memory-safe implementation\")\n",
    "        if '///' in example['rust_code'] or '/*' in example['rust_code']:\n",
    "            indicators.append(\"• Documentation comments\")\n",
    "        \n",
    "        for indicator in indicators:\n",
    "            print(f\"   {indicator}\")\n",
    "        \n",
    "        if i < 3:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "def compare_translation_approaches():\n",
    "    \"\"\"\n",
    "    Compare different translation approaches on the same C code\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\nTRANSLATION APPROACH COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"No dataset available for comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Take the highest quality example\n",
    "    best_example = max(dataset, key=lambda x: x['quality_score'])\n",
    "    \n",
    "    print(f\"SOURCE: {best_example['project']} - {best_example['c_file']}\")\n",
    "    print(f\"ORIGINAL C CODE:\")\n",
    "    print(\"─\" * 40)\n",
    "    print(best_example['c_code'])\n",
    "    \n",
    "    print(f\"\\nEXPERT RUST TRANSLATION:\")\n",
    "    print(\"─\" * 40)\n",
    "    print(best_example['rust_code'])\n",
    "    \n",
    "    print(f\"\\nWHAT A FINE-TUNED MODEL SHOULD LEARN:\")\n",
    "    print(\"─\" * 40)\n",
    "    \n",
    "    # Analyze key transformations\n",
    "    c_code = best_example['c_code']\n",
    "    rust_code = best_example['rust_code']\n",
    "    \n",
    "    transformations = []\n",
    "    \n",
    "    if 'malloc' in c_code.lower():\n",
    "        transformations.append(\"• malloc/free → Vec<T> or Box<T> (automatic memory management)\")\n",
    "    if 'NULL' in c_code:\n",
    "        transformations.append(\"• NULL checks → Option<T> (type-safe nullability)\")\n",
    "    if 'struct' in c_code.lower():\n",
    "        transformations.append(\"• C structs → Rust structs with ownership semantics\")\n",
    "    if any(word in rust_code.lower() for word in ['pub fn', 'impl']):\n",
    "        transformations.append(\"• C functions → Rust methods with proper visibility\")\n",
    "    if 'Result<' in rust_code:\n",
    "        transformations.append(\"• Error codes → Result<T, E> (explicit error handling)\")\n",
    "    \n",
    "    for transformation in transformations:\n",
    "        print(f\"   {transformation}\")\n",
    "\n",
    "def show_quality_distribution():\n",
    "    \"\"\"\n",
    "    Show the distribution of quality scores across all examples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\nQUALITY SCORE DISTRIBUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"No dataset available for analysis.\")\n",
    "        return\n",
    "    \n",
    "    scores = [ex['quality_score'] for ex in dataset]\n",
    "    \n",
    "    print(f\"STATISTICS:\")\n",
    "    print(f\"   Total Examples: {len(scores)}\")\n",
    "    print(f\"   Average Quality: {np.mean(scores):.2f}/10\")\n",
    "    print(f\"   Highest Quality: {max(scores):.1f}/10\")\n",
    "    print(f\"   Lowest Quality: {min(scores):.1f}/10\")\n",
    "    print(f\"   Standard Deviation: {np.std(scores):.2f}\")\n",
    "    \n",
    "    # Create quality bins\n",
    "    bins = [0, 2, 4, 6, 8, 10]\n",
    "    labels = ['Poor (0-2)', 'Below Avg (2-4)', 'Average (4-6)', 'Good (6-8)', 'Excellent (8-10)']\n",
    "    \n",
    "    print(f\"\\nQUALITY DISTRIBUTION:\")\n",
    "    for i in range(len(bins)-1):\n",
    "        count = sum(1 for score in scores if bins[i] <= score < bins[i+1])\n",
    "        percentage = (count / len(scores)) * 100\n",
    "        bar = \"█\" * max(1, int(percentage / 5))\n",
    "        print(f\"   {labels[i]:<15} │{bar:<20} {count:2d} ({percentage:4.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nINSIGHT:\")\n",
    "    high_quality_count = sum(1 for score in scores if score >= 6)\n",
    "    if high_quality_count > 0:\n",
    "        print(f\"   {high_quality_count} examples ({(high_quality_count/len(scores)*100):.1f}%) are high-quality (6+)\")\n",
    "        print(f\"   These form the core of our training dataset\")\n",
    "    else:\n",
    "        print(f\"   Limited high-quality examples - would benefit from data augmentation\")\n",
    "\n",
    "# Execute all example displays\n",
    "display_high_quality_examples()\n",
    "compare_translation_approaches()\n",
    "show_quality_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d41528b-e384-420d-8cd0-cee19d7f639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting 25 examples for O3 reasoning model...\n",
      "\n",
      "Dataset split for O3:\n",
      "   Training examples: 20\n",
      "   Validation examples: 5\n",
      "Saved 20 examples to finetune_outputs/train_o3_crust.jsonl\n",
      "Saved 5 examples to finetune_outputs/val_o3_crust.jsonl\n",
      "\n",
      "Example formatted for O3:\n",
      "System: You are an expert C-to-Rust translator with deep understanding of systems programming and memory safety.\n",
      "\n",
      "REASONING APPROACH:\n",
      "1. First analyze the C c...\n",
      "User: Translate this C code to safe, idiomatic Rust using step-by-step reasoning:\n",
      "\n",
      "**CONTEXT:**\n",
      "- Project: rubiksolver\n",
      "- Source: rubik_model_tests.c → hash.rs\n",
      "- Complexity: 68 lines of C code\n",
      "\n",
      "**C CODE TO T...\n",
      "Assistant: **ANALYSIS:**\n",
      "- Manual memory allocation/deallocation detected\n",
      "\n",
      "**STRATEGY:**\n",
      "- Add comprehensive error handling\\n- Ensure thread safety where applicable\\n- Use Rust naming conventions (snake_case)\n",
      "\n",
      "*...\n"
     ]
    }
   ],
   "source": [
    "def format_for_o3_instruction_tuning(examples):\n",
    "    \"\"\"\n",
    "    Enhanced formatting for O3's reasoning capabilities\n",
    "    Optimized for step-by-step thinking and explicit reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Formatting {len(examples)} examples for O3 reasoning model...\")\n",
    "    \n",
    "    # Enhanced system prompt for O3's reasoning\n",
    "    system_prompt = \"\"\"You are an expert C-to-Rust translator with deep understanding of systems programming and memory safety.\n",
    "\n",
    "REASONING APPROACH:\n",
    "1. First analyze the C code structure and identify key patterns\n",
    "2. Plan the Rust translation strategy step-by-step\n",
    "3. Consider safety implications and choose appropriate Rust types\n",
    "4. Implement with clear explanations of design decisions\n",
    "\n",
    "EXPERTISE AREAS:\n",
    "- Memory management: malloc/free → Vec<T>, Box<T>, Rc<T>, Arc<T>\n",
    "- Error handling: NULL checks → Result<T, E>, Option<T>\n",
    "- Ownership: Manual memory management → Rust ownership system\n",
    "- Safety: Buffer overflows → bounds checking, safe indexing\n",
    "- Concurrency: Raw pointers → safe abstractions (Mutex, RwLock)\n",
    "- API design: C functions → Rust methods, traits, and modules\n",
    "\n",
    "TRANSLATION PRINCIPLES:\n",
    "- Maintain functional equivalence\n",
    "- Leverage Rust's type system for safety\n",
    "- Use idiomatic Rust patterns\n",
    "- Provide clear documentation\n",
    "- Handle edge cases explicitly\"\"\"\n",
    "\n",
    "    formatted_examples = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # O3-optimized user prompt with reasoning structure\n",
    "        user_prompt = f\"\"\"Translate this C code to safe, idiomatic Rust using step-by-step reasoning:\n",
    "\n",
    "**CONTEXT:**\n",
    "- Project: {example['project']}\n",
    "- Source: {example['c_file']} → {example['rust_file']}\n",
    "- Complexity: {example['c_lines']} lines of C code\n",
    "\n",
    "**C CODE TO TRANSLATE:**\n",
    "```c\n",
    "{example['c_code']}\n",
    "```\n",
    "\n",
    "**TRANSLATION REQUIREMENTS:**\n",
    "1. Maintain all original functionality\n",
    "2. Use Rust's memory safety features (no unsafe blocks unless absolutely necessary)\n",
    "3. Follow Rust naming conventions (snake_case for functions/variables)\n",
    "4. Implement proper error handling with Result<T, E> or Option<T>\n",
    "5. Add appropriate documentation comments\n",
    "6. Ensure thread safety where applicable\n",
    "\n",
    "**PLEASE PROVIDE:**\n",
    "1. **Analysis**: Key patterns and challenges in the C code\n",
    "2. **Strategy**: Rust types and patterns you'll use\n",
    "3. **Implementation**: Final Rust code with comments\"\"\"\n",
    "\n",
    "        # Enhanced assistant response format for O3\n",
    "        rust_analysis = analyze_c_code_patterns(example['c_code'])\n",
    "        rust_strategy = plan_rust_translation(example['c_code'])\n",
    "        \n",
    "        assistant_response = f\"\"\"**ANALYSIS:**\n",
    "{rust_analysis}\n",
    "\n",
    "**STRATEGY:**\n",
    "{rust_strategy}\n",
    "\n",
    "**IMPLEMENTATION:**\n",
    "```rust\n",
    "{example['rust_code']}\n",
    "```\n",
    "\n",
    "**KEY IMPROVEMENTS:**\n",
    "- Eliminated manual memory management\n",
    "- Added proper error handling\n",
    "- Ensured memory safety through Rust's type system\n",
    "- Maintained API compatibility while improving safety\"\"\"\n",
    "\n",
    "        formatted_example = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        formatted_examples.append(formatted_example)\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "def analyze_c_code_patterns(c_code):\n",
    "    \"\"\"Analyze C code to identify key patterns for O3 reasoning\"\"\"\n",
    "    patterns = []\n",
    "    \n",
    "    if 'malloc' in c_code or 'free' in c_code:\n",
    "        patterns.append(\"Manual memory allocation/deallocation detected\")\n",
    "    if 'NULL' in c_code:\n",
    "        patterns.append(\"NULL pointer checks present - candidate for Option<T>\")\n",
    "    if 'struct' in c_code:\n",
    "        patterns.append(\"C structs - will become Rust structs with ownership\")\n",
    "    if 'typedef' in c_code:\n",
    "        patterns.append(\"Type definitions - will use Rust type aliases or newtypes\")\n",
    "    if '*' in c_code and 'char' in c_code:\n",
    "        patterns.append(\"C strings detected - consider String or &str\")\n",
    "    \n",
    "    return \"\\\\n\".join(f\"- {pattern}\" for pattern in patterns) if patterns else \"- Standard C code patterns\"\n",
    "\n",
    "def plan_rust_translation(c_code):\n",
    "    \"\"\"Plan Rust translation strategy for O3 reasoning\"\"\"\n",
    "    strategies = []\n",
    "    \n",
    "    if 'malloc' in c_code:\n",
    "        strategies.append(\"Replace malloc/free with Vec<T> or Box<T> for automatic memory management\")\n",
    "    if 'NULL' in c_code:\n",
    "        strategies.append(\"Use Option<T> for nullable pointers and Result<T, E> for error handling\")\n",
    "    if 'struct' in c_code:\n",
    "        strategies.append(\"Convert C structs to Rust structs with proper ownership semantics\")\n",
    "    if 'typedef' in c_code:\n",
    "        strategies.append(\"Use Rust type aliases or newtype patterns for type safety\")\n",
    "    \n",
    "    strategies.append(\"Add comprehensive error handling\")\n",
    "    strategies.append(\"Ensure thread safety where applicable\")\n",
    "    strategies.append(\"Use Rust naming conventions (snake_case)\")\n",
    "    \n",
    "    return \"\\\\n\".join(f\"- {strategy}\" for strategy in strategies)\n",
    "\n",
    "\n",
    "# Format examples\n",
    "formatted_data = format_for_o3_instruction_tuning(dataset)\n",
    "\n",
    "# Split into train/validation\n",
    "if len(formatted_data) >= 2:\n",
    "    split_idx = max(1, int(len(formatted_data) * CONFIG['train_split']))\n",
    "    train_data = formatted_data[:split_idx]\n",
    "    val_data = formatted_data[split_idx:]\n",
    "else:\n",
    "    train_data = formatted_data\n",
    "    val_data = []\n",
    "\n",
    "print(f\"\\nDataset split for O3:\")\n",
    "print(f\"   Training examples: {len(train_data)}\")\n",
    "print(f\"   Validation examples: {len(val_data)}\")\n",
    "\n",
    "# Save formatted data\n",
    "def save_jsonl(data, filepath):\n",
    "    \"\"\"Save data in JSONL format\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for example in data:\n",
    "            f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved {len(data)} examples to {filepath}\")\n",
    "\n",
    "train_file = f\"{CONFIG['output_dir']}/train_o3_crust.jsonl\"\n",
    "val_file = f\"{CONFIG['output_dir']}/val_o3_crust.jsonl\"\n",
    "\n",
    "save_jsonl(train_data, train_file)\n",
    "if val_data:\n",
    "    save_jsonl(val_data, val_file)\n",
    "\n",
    "# Display example\n",
    "print(f\"\\nExample formatted for O3:\")\n",
    "if train_data:\n",
    "    example = train_data[0]\n",
    "    print(f\"System: {example['messages'][0]['content'][:150]}...\")\n",
    "    print(f\"User: {example['messages'][1]['content'][:200]}...\")\n",
    "    print(f\"Assistant: {example['messages'][2]['content'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7b34f5-a19e-4681-b7fb-3623b021c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O3 COST ANALYSIS\n",
      "========================================\n",
      "Token breakdown:\n",
      "   Total examples: 25\n",
      "   Total tokens: 32,184\n",
      "   Average tokens per example: 1,287\n",
      "\n",
      "Cost estimates per 1K tokens:\n",
      "Model                Training     Inference    Total (3 epochs)\n",
      "-----------------------------------------------------------------\n",
      "o3-mini (optimistic) $0.50        $0.15        $48.28         \n",
      "o3-mini (realistic)  $2.00        $0.50        $193.10        \n",
      "o3 (expensive)       $10.00       $2.00        $965.52        \n",
      "\n",
      "RECOMMENDED CONFIGURATION:\n",
      "   Model: o3-mini\n",
      "   Examples: 25\n",
      "   Epochs: 1 (cost control)\n",
      "   Estimated cost: $64.37\n",
      "\n",
      "O3 FINE-TUNING CONFIGURATION\n",
      "========================================\n",
      "Configuration:\n",
      "   model: gpt-4o-mini-2024-07-18\n",
      "   training_file: finetune_outputs/train_o3_crust.jsonl\n",
      "   validation_file: finetune_outputs/val_o3_crust.jsonl\n",
      "   hyperparameters:\n",
      "     n_epochs: 1\n",
      "     learning_rate_multiplier: 0.1\n",
      "     batch_size: 1\n",
      "   suffix: crust-o3-1751062164\n",
      "   cost_controls:\n",
      "     max_cost_limit: 50.0\n",
      "     early_stopping: True\n",
      "     monitor_cost: True\n"
     ]
    }
   ],
   "source": [
    "def analyze_o3_costs():\n",
    "    \"\"\"\n",
    "    Detailed cost analysis for O3 fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"O3 COST ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Rough token calculation\n",
    "    total_tokens = 0\n",
    "    for example in formatted_data:\n",
    "        for message in example['messages']:\n",
    "            total_tokens += len(message['content']) // 4\n",
    "    \n",
    "    print(f\"Token breakdown:\")\n",
    "    print(f\"   Total examples: {len(formatted_data)}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Average tokens per example: {total_tokens // len(formatted_data) if formatted_data else 0:,}\")\n",
    "    \n",
    "    # O3 cost estimates\n",
    "    cost_scenarios = {\n",
    "        'o3-mini (optimistic)': {'training': 0.50, 'inference': 0.15},\n",
    "        'o3-mini (realistic)': {'training': 2.00, 'inference': 0.50}, \n",
    "        'o3 (expensive)': {'training': 10.00, 'inference': 2.00}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCost estimates per 1K tokens:\")\n",
    "    print(f\"{'Model':<20} {'Training':<12} {'Inference':<12} {'Total (3 epochs)':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for model, costs in cost_scenarios.items():\n",
    "        training_cost_total = (total_tokens * 3 * costs['training']) / 1000\n",
    "        inference_cost = costs['inference']\n",
    "        \n",
    "        print(f\"{model:<20} ${costs['training']:<11.2f} ${inference_cost:<11.2f} ${training_cost_total:<14.2f}\")\n",
    "    \n",
    "    # Recommendation\n",
    "    estimated_cost = (total_tokens * 3 * 2.00) / 1000\n",
    "    \n",
    "    print(f\"\\nRECOMMENDED CONFIGURATION:\")\n",
    "    print(f\"   Model: o3-mini\")\n",
    "    print(f\"   Examples: {len(formatted_data)}\")\n",
    "    print(f\"   Epochs: 1 (cost control)\")\n",
    "    print(f\"   Estimated cost: ${estimated_cost/3:.2f}\")\n",
    "    \n",
    "    return estimated_cost\n",
    "\n",
    "estimated_cost = analyze_o3_costs()\n",
    "\n",
    "def create_o3_job_config():\n",
    "    \"\"\"\n",
    "    Create O3 fine-tuning job configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nO3 FINE-TUNING CONFIGURATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Conservative config for O3\n",
    "    job_config = {\n",
    "        'model': CONFIG['target_model'],\n",
    "        'training_file': train_file,\n",
    "        'validation_file': val_file if val_data else None,\n",
    "        'hyperparameters': {\n",
    "            'n_epochs': 1,\n",
    "            'learning_rate_multiplier': 0.1,\n",
    "            'batch_size': 1\n",
    "        },\n",
    "        'suffix': f'crust-o3-{int(time.time())}',\n",
    "        'cost_controls': {\n",
    "            'max_cost_limit': 50.0,\n",
    "            'early_stopping': True,\n",
    "            'monitor_cost': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    for key, value in job_config.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"   {key}:\")\n",
    "            for subkey, subvalue in value.items():\n",
    "                print(f\"     {subkey}: {subvalue}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return job_config\n",
    "\n",
    "o3_config = create_o3_job_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f26b72e-c5d3-4632-9bba-c4ba94f426b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating working fine-tuning configuration...\n",
      "\n",
      "WORKING FINE-TUNING CONFIGURATION\n",
      "=============================================\n",
      "✓ Using REAL model: gpt-4o-mini-2024-07-18\n",
      "✓ Valid hyperparameters only\n",
      "✓ Compatible with OpenAI API\n",
      "✓ Output directory: o3_fine_tuning_outputs\n",
      "\n",
      "Configuration:\n",
      "   model: gpt-4o-mini-2024-07-18\n",
      "   training_file: o3_fine_tuning_outputs/train_o3_crust.jsonl\n",
      "   validation_file: o3_fine_tuning_outputs/val_o3_crust.jsonl\n",
      "   output_dir: o3_fine_tuning_outputs\n",
      "   hyperparameters:\n",
      "     n_epochs: 3\n",
      "     learning_rate_multiplier: 2.0\n",
      "     batch_size: auto\n",
      "   suffix: crust-917256\n",
      "\n",
      "REAL API EXECUTION\n",
      "==============================\n",
      "Uploading training file...\n",
      "Training file uploaded: file-WUUuiGUNEak78aD7WQARMy\n",
      "Uploading validation file...\n",
      "Validation file uploaded: file-7emUVJEFwhnVN2rZoJ8NZJ\n",
      "Creating fine-tuning job...\n",
      "Fine-tuning job created successfully!\n",
      "   Job ID: ftjob-r5JVflLgD5nLf10rqsnpe0Pu\n",
      "   Status: validating_files\n",
      "   Model: gpt-4o-mini-2024-07-18\n",
      "Job info saved to: o3_fine_tuning_outputs\\real_job_info.json\n"
     ]
    }
   ],
   "source": [
    "def create_working_job_config(train_file, val_file=None, output_dir=\"./\"):\n",
    "    \"\"\"\n",
    "    Create a WORKING fine-tuning job configuration that actually works with OpenAI API\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nWORKING FINE-TUNING CONFIGURATION\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    available_models = [\n",
    "        'gpt-4o-mini-2024-07-18',\n",
    "        'gpt-4o-2024-08-06',         # Full GPT-4o (expensive)\n",
    "        'gpt-3.5-turbo-0125',        # Legacy fallback\n",
    "        'gpt-3.5-turbo-1106',        # Older legacy\n",
    "    ]\n",
    "    \n",
    "    target_model = 'gpt-4o-mini-2024-07-18'\n",
    "\n",
    "    timestamp = str(int(time.time()))[-6:]\n",
    "    \n",
    "    # VALID OpenAI API configuration\n",
    "    job_config = {\n",
    "        'model': target_model,\n",
    "        'training_file': train_file,\n",
    "        'validation_file': val_file if val_file else None,\n",
    "        'output_dir': output_dir,\n",
    "        'hyperparameters': {\n",
    "            'n_epochs': 3,\n",
    "            'learning_rate_multiplier': 2.0,\n",
    "            'batch_size': 'auto'\n",
    "        },\n",
    "        'suffix': f'crust-{timestamp}'\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Using REAL model: {target_model}\")\n",
    "    print(f\"✓ Valid hyperparameters only\")\n",
    "    print(f\"✓ Compatible with OpenAI API\")\n",
    "    print(f\"✓ Output directory: {output_dir}\")\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    for key, value in job_config.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"   {key}:\")\n",
    "            for subkey, subvalue in value.items():\n",
    "                print(f\"     {subkey}: {subvalue}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return job_config\n",
    "\n",
    "def execute_real_fine_tuning(config, mock_mode=True):\n",
    "    \"\"\"\n",
    "    Execute REAL fine-tuning job that actually works with OpenAI\n",
    "    \"\"\"\n",
    "    \n",
    "    if mock_mode:\n",
    "        print(f\"\\nMOCK EXECUTION (Demo Mode)\")\n",
    "        print(\"=\" * 40)\n",
    "        print(\"This shows what WOULD happen with a real API call\")\n",
    "        \n",
    "        mock_job = {\n",
    "            'id': 'ftjob-working-demo-12345',\n",
    "            'object': 'fine_tuning.job',\n",
    "            'model': config['model'],\n",
    "            'created_at': int(time.time()),\n",
    "            'training_file': config['training_file'],\n",
    "            'validation_file': config.get('validation_file'),\n",
    "            'hyperparameters': config['hyperparameters'],\n",
    "            'suffix': config['suffix'],\n",
    "            'status': 'validating_files'\n",
    "        }\n",
    "        \n",
    "        print(f\"Mock job created: {mock_job['id']}\")\n",
    "        print(f\"Status: {mock_job['status']}\")\n",
    "        return mock_job\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nREAL API EXECUTION\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if not OPENAI_AVAILABLE:\n",
    "            print(\"OpenAI package not available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "            \n",
    "            # Upload training file and get file ID\n",
    "            print(\"Uploading training file...\")\n",
    "            with open(config['training_file'], 'rb') as f:\n",
    "                train_file_obj = client.files.create(file=f, purpose='fine-tune')\n",
    "            \n",
    "            print(f\"Training file uploaded: {train_file_obj.id}\")\n",
    "            \n",
    "            # Upload validation file if provided\n",
    "            val_file_id = None\n",
    "            if config.get('validation_file'):\n",
    "                print(\"Uploading validation file...\")\n",
    "                with open(config['validation_file'], 'rb') as f:\n",
    "                    val_file_obj = client.files.create(file=f, purpose='fine-tune')\n",
    "                val_file_id = val_file_obj.id\n",
    "                print(f\"Validation file uploaded: {val_file_id}\")\n",
    "            \n",
    "            print(\"Creating fine-tuning job...\")\n",
    "            \n",
    "            job_params = {\n",
    "                'training_file': train_file_obj.id,\n",
    "                'model': config['model'],\n",
    "                'hyperparameters': config['hyperparameters'],\n",
    "                'suffix': config['suffix']\n",
    "            }\n",
    "            \n",
    "            if val_file_id:\n",
    "                job_params['validation_file'] = val_file_id\n",
    "            \n",
    "            job = client.fine_tuning.jobs.create(**job_params)\n",
    "            \n",
    "            print(f\"Fine-tuning job created successfully!\")\n",
    "            print(f\"   Job ID: {job.id}\")\n",
    "            print(f\"   Status: {job.status}\")\n",
    "            print(f\"   Model: {job.model}\")\n",
    "            \n",
    "            # Save job info with proper error handling\n",
    "            job_info = {\n",
    "                'job_id': job.id,\n",
    "                'created_at': job.created_at,\n",
    "                'model': job.model,\n",
    "                'training_file_id': job.training_file,\n",
    "                'validation_file_id': getattr(job, 'validation_file', None),\n",
    "                'status': job.status,\n",
    "                'hyperparameters': job.hyperparameters,\n",
    "                'suffix': config['suffix'],\n",
    "                'original_config': config\n",
    "            }\n",
    "            \n",
    "            # Save with error handling\n",
    "            try:\n",
    "                output_dir = config.get('output_dir', './')\n",
    "                os.makedirs(output_dir, exist_ok=True)  # Create directory if needed\n",
    "                output_file = os.path.join(output_dir, \"real_job_info.json\")\n",
    "                \n",
    "                with open(output_file, 'w') as f:\n",
    "                    json.dump(job_info, f, indent=2, default=str)\n",
    "                \n",
    "                print(f\"Job info saved to: {output_file}\")\n",
    "            except Exception as save_error:\n",
    "                print(f\"Job created successfully but couldn't save info: {save_error}\")\n",
    "                print(f\"   Job ID: {job.id} (save this!)\")\n",
    "            \n",
    "            return job\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating job: {e}\")\n",
    "            print(\"Common issues:\")\n",
    "            print(\"- Invalid API key\")\n",
    "            print(\"- Insufficient account tier (need Tier 1+ for GPT-4o mini)\")\n",
    "            print(\"- File not properly formatted (must be JSONL)\")\n",
    "            print(\"- Invalid model name\")\n",
    "            print(\"- Suffix too long (max 18 chars)\")\n",
    "            return None\n",
    "\n",
    "print(\"Creating working fine-tuning configuration...\")\n",
    "\n",
    "train_file = \"o3_fine_tuning_outputs/train_o3_crust.jsonl\"\n",
    "val_file = \"o3_fine_tuning_outputs/val_o3_crust.jsonl\"\n",
    "output_dir = \"o3_fine_tuning_outputs\"\n",
    "\n",
    "working_config = create_working_job_config(train_file, val_file, output_dir)\n",
    "result = execute_real_fine_tuning(working_config, mock_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905fa8f1-b49b-42ba-b774-cfb0f4580657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating your fine-tuning job...\n",
      "Need 'result' from execute_real_fine_tuning() and 'dataset'\n",
      "Usage: evaluate_model(your_job_result, your_dataset)\n"
     ]
    }
   ],
   "source": [
    "def monitor_job_status(job):\n",
    "    \"\"\"Check fine-tuning job status and return model ID if complete\"\"\"\n",
    "    \n",
    "    job_id = job.id if hasattr(job, 'id') else job.get('id')\n",
    "    if not job_id:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        current_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        \n",
    "        print(f\"Job {job_id}: {current_job.status}\")\n",
    "        \n",
    "        if current_job.status == 'succeeded':\n",
    "            print(f\"Model ready: {current_job.fine_tuned_model}\")\n",
    "            return current_job.fine_tuned_model\n",
    "        elif current_job.status == 'failed':\n",
    "            print(f\"Job failed: {getattr(current_job, 'error', 'Unknown error')}\")\n",
    "            return None\n",
    "        else:\n",
    "            return 'in_progress'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking status: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_model(model_id, test_examples, max_tests=3):\n",
    "    \"\"\"Test fine-tuned model with C-to-Rust translation\"\"\"\n",
    "    \n",
    "    if not model_id or model_id == 'in_progress':\n",
    "        print(\"Model not ready\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nTesting model: {model_id}\")\n",
    "    \n",
    "    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    results = []\n",
    "    \n",
    "    for i, example in enumerate(test_examples[:max_tests], 1):\n",
    "        print(f\"\\n--- Test {i}: {example.get('project', 'Unknown')} ---\")\n",
    "        \n",
    "        c_code = example.get('c_code', '')\n",
    "        print(f\"C Code: {c_code[:100]}...\")\n",
    "        \n",
    "        prompt = f\"\"\"Translate this C code to safe, idiomatic Rust:\n",
    "\n",
    "```c\n",
    "{c_code}\n",
    "```\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert C-to-Rust translator.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1500,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            generated_rust = response.choices[0].message.content\n",
    "            print(f\"Generated: {generated_rust[:100]}...\")\n",
    "            \n",
    "            # Quick evaluation\n",
    "            quality = evaluate_quality(generated_rust)\n",
    "            compiles = check_compilation(generated_rust)\n",
    "            \n",
    "            print(f\"Quality: {quality:.2f}, Compiles: {compiles}\")\n",
    "            \n",
    "            results.append({\n",
    "                'project': example.get('project'),\n",
    "                'quality': quality,\n",
    "                'compiles': compiles,\n",
    "                'generated_rust': generated_rust\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results.append({'project': example.get('project'), 'error': str(e)})\n",
    "    \n",
    "    # Summary\n",
    "    if results:\n",
    "        valid = [r for r in results if 'quality' in r]\n",
    "        if valid:\n",
    "            avg_quality = sum(r['quality'] for r in valid) / len(valid)\n",
    "            compile_rate = sum(r['compiles'] for r in valid) / len(valid)\n",
    "            \n",
    "            print(f\"\\nSummary: Quality {avg_quality:.2f}/1.0, Compiles {compile_rate:.1%}\")\n",
    "            \n",
    "            if avg_quality >= 0.6:\n",
    "                print(\"Excellent performance!\")\n",
    "            elif avg_quality >= 0.4:\n",
    "                print(\"Good results\")\n",
    "            else:\n",
    "                print(\"Needs improvement\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_quality(rust_code):\n",
    "    \"\"\"Quick quality score for Rust code\"\"\"\n",
    "    \n",
    "    rust_lower = rust_code.lower()\n",
    "    score = 0\n",
    "    \n",
    "    # Positive patterns\n",
    "    good_patterns = ['vec<', 'result<', 'option<', '?', 'impl', 'pub fn', 'use std']\n",
    "    for pattern in good_patterns:\n",
    "        if pattern in rust_lower:\n",
    "            score += 0.15\n",
    "    \n",
    "    # Negative patterns\n",
    "    bad_patterns = ['unsafe', '.unwrap()', 'panic!', '*mut', 'malloc']\n",
    "    for pattern in bad_patterns:\n",
    "        if pattern in rust_lower:\n",
    "            score -= 0.2\n",
    "    \n",
    "    return max(0, min(1, score))\n",
    "\n",
    "def check_compilation(rust_code):\n",
    "    \"\"\"Check if Rust code compiles\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.rs', delete=False) as f:\n",
    "            if 'fn main(' not in rust_code:\n",
    "                full_code = f\"{rust_code}\\n\\nfn main() {{}}\\n\"\n",
    "            else:\n",
    "                full_code = rust_code\n",
    "            \n",
    "            f.write(full_code)\n",
    "            temp_file = f.name\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            ['rustc', '--crate-type', 'bin', temp_file, '-o', '/dev/null'],\n",
    "            capture_output=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        os.unlink(temp_file)\n",
    "        return result.returncode == 0\n",
    "        \n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def evaluate_model(job, dataset):\n",
    "    \"\"\"Complete evaluation: check job → test model → results\"\"\"\n",
    "    \n",
    "    print(\"Fine-tuned Model Evaluation\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Check job status\n",
    "    model_id = monitor_job_status(job)\n",
    "    \n",
    "    if model_id == 'in_progress':\n",
    "        print(\"Job still running. Try again later.\")\n",
    "        return None\n",
    "    \n",
    "    if not model_id:\n",
    "        print(\"Job failed or not found\")\n",
    "        return None\n",
    "    \n",
    "    # Test the model\n",
    "    test_examples = dataset[-3:] if len(dataset) >= 3 else dataset\n",
    "    results = test_model(model_id, test_examples)\n",
    "    \n",
    "    # Save results\n",
    "    report = {\n",
    "        'model_id': model_id,\n",
    "        'timestamp': time.time(),\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Results saved to evaluation_results.json\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(\"Evaluating your fine-tuning job...\")\n",
    "\n",
    "try:\n",
    "    report = evaluate_model(result, dataset)\n",
    "    \n",
    "    if report:\n",
    "        print(f\"\\nEvaluation complete!\")\n",
    "        print(f\"Model: {report['model_id']}\")\n",
    "    else:\n",
    "        print(f\"\\nCheck again later when job completes\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"Need 'result' from execute_real_fine_tuning() and 'dataset'\")\n",
    "    print(\"Usage: evaluate_model(your_job_result, your_dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f4285-6b26-489a-a135-e1994bb64fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
